{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# python setup for qgis processing\n",
    "\n",
    "import sys\n",
    "from qgis.core import *\n",
    "# need qgis,gui ?\n",
    "#from qgis.gui import *\n",
    "# need PyQt4.QtCore ?\n",
    "#from PyQt4.QtCore import *\n",
    "from PyQt4.QtGui import *\n",
    "\n",
    "# what does True refer to below ?\n",
    "app = QApplication([], True)\n",
    "QgsApplication.setPrefixPath(\"/usr\", True)\n",
    "# /usr correct?\n",
    "#QgsApplication.setPrefixPath(qgis_path, True)\n",
    "QgsApplication.initQgis()\n",
    "\n",
    "sys.path.append('/usr/share/qgis/python/plugins')\n",
    "from processing.core.Processing import Processing\n",
    "Processing.initialize()\n",
    "from processing.tools import *\n",
    "\n",
    "#why is this still needed\n",
    "import processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# description\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#!/usr/bin/python\n",
    "# TunnelExcavationData.py\n",
    "\n",
    "# Python procedure for TunnelGIS Engineering App\n",
    "# Author: KK\n",
    "# Date: 01.04.2017\n",
    "\n",
    "# Purpose of this procedure:\n",
    "# 1. Prepare input data ....\n",
    "# 2.\n",
    "# 3.\n",
    "# 4. \n",
    "# 5. \n",
    "\n",
    "# This python routine is a script, intended to guide the user through the described procedure.\n",
    "# As a script, the procedure does not generally include data validation and error handling.\n",
    "# Users are expected to understand and adjust the code as needed for their application.\n",
    "\n",
    "# Required Input Files:\n",
    "#   DEM with surface topography\n",
    "#   DEM with rock surface\n",
    "#   stationed tunnel alignment\n",
    "#   tunnel layout data\n",
    "\n",
    "# References:\n",
    "# http://gis.stackexchange.com/questions/197825/how-to-convert-multiple-csv-files-to-shp-using-python-and-no-arcpy\n",
    "# to get grass help:   processing.alghelp(\"grass7:r.what.points\")\n",
    "\n",
    "# IMPORTANT: requires qgis setup before running this procedure\n",
    "# run ./pyqgis.sh from command line before starting python (or set up IDE accordingly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# import required libraries\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import shapely as sp\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# set wd for this procedure \n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "os.chdir(\"/home/kaelin_joseph/TunnelGIS/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# define input files\n",
    "# ----------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Ostroehre\n",
    "# #  use either Ostroehre or Westroehre, comment out the data set not used\n",
    "\n",
    "# DTM = \"data/in/swissALTI3D_.tif\"  \n",
    "# RockSurface = \"data/in/Felsisohypsen-raster.tif\"            \n",
    "# AlignmentData = \"data/in/Ostroehre.AlignmentData.R32.csv\"  #revised 2018.02.28\n",
    "# LayoutData = \"data/in/Ostroehre.TunnelLayoutData.R31.csv\"  #revised 2018.03.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Westroehre\n",
    "#  use either Ostroehre or Westroehre, comment out the data set not used\n",
    "\n",
    "DTM = \"data/in/swissALTI3D_.tif\"  \n",
    "RockSurface = \"data/in/Felsisohypsen-raster.tif\"            \n",
    "AlignmentData = \"data/in/Westroehre.AlignmentData.R2.csv\"  #revised 2018.03.12\n",
    "LayoutData = \"data/in/Westroehre.TunnelLayoutData.R2.csv\"  #revised 2018.03.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# define output files\n",
    "# ----------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Ostroehre\n",
    "# #  use either Ostroehre or Westroehre, comment out the data set not used\n",
    "\n",
    "# TunnelExcavationData = \"data/out/Ostroehre.TunnelExcavationData.R3.csv\"\n",
    "# # headers: Station, Easting, Northing, Elevation, DTM, RockSurface, StationReal, RockCover,\n",
    "# #          WBScode, WorkType, ExcavationType, ProfileType, SectionArea, Description,\n",
    "# #          BoreClass, SupportClass, DisposalClass, StationInterval, ExcavationVolume, DisposalVolume\n",
    "# Alignment_SHP ='data/out/Ostroehre.Alignment.R3.shp'\n",
    "# BoQ = \"data/out/Ostroehre.TunnelBoQdata.R3.csv\"\n",
    "# # temporary data\n",
    "# Alignment_DTM = \"data/out/Ostroehre.Terrain.R3.csv\"\n",
    "# Alignment_RockSurface = \"data/out/Ostroehre.RockSurface.R3.csv\"  # JK ToDo: RockSurface?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Westroehre\n",
    "#  use either Ostroehre or Westroehre, comment out the data set not used\n",
    "\n",
    "TunnelExcavationData = \"data/out/Westroehre.TunnelExcavationData.R2.csv\"\n",
    "# headers: Station, Easting, Northing, Elevation, DTM, RockSurface, StationReal, RockCover,\n",
    "#          WBScode, WorkType, ExcavationType, ProfileType, SectionArea, Description,\n",
    "#          BoreClass, SupportClass, DisposalClass, StationInterval, ExcavationVolume, DisposalVolume\n",
    "Alignment_SHP ='data/out/Westroehre.Alignment.R2.shp'\n",
    "BoQ = \"data/out/Westroehre.TunnelBoQdata.R2.csv\"\n",
    "# temporary data\n",
    "Alignment_DTM = \"data/out/Westroehre.Terrain.R2.csv\"\n",
    "Alignment_RockSurface = \"data/out/Westroehre.RockSurface.R2.csv\"  # JK ToDo: RockSurface?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# define Bore Classes\n",
    "#   define Bore Classes as class, to separate definition of methods from execution\n",
    "#   class method is used as a modifier to the TunnelExcavationData (dataframe) class.\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# better to add mtethods .bc1, .bc2, .bc3 to TunnExcvDf ??\n",
    "\n",
    "tunn_h =13.0   # define tunnel height\n",
    "volume_unit='m3'  # unit to be used for volume calculation and reporting\n",
    "\n",
    "class BoreClass:\n",
    "    \"\"\"Determine Bore Class for TBM tunnels\"\"\"\n",
    "    # BC1 - tunnel predominantly in soil\n",
    "    def bc1(self):\n",
    "        TunnExcvDF.loc[(TunnExcvDF[\"ExcavationType\"] == \"TBM\") & \n",
    "        (TunnExcvDF[\"RockSurface\"] <= TunnExcvDF[\"Elevation\"] -tunn_h*0.25),\"BoreClass\"] \\\n",
    "        =\"BC1\"\n",
    "    # BC2 - tunnel with mixed face\n",
    "    def bc2(self):\n",
    "        TunnExcvDF.loc[(TunnExcvDF[\"ExcavationType\"] == \"TBM\") & \n",
    "        (TunnExcvDF[\"RockSurface\"] > TunnExcvDF[\"Elevation\"] -tunn_h*0.25) & \n",
    "        (TunnExcvDF[\"RockSurface\"] < TunnExcvDF[\"Elevation\"] +tunn_h/2.0 +1.5),\"BoreClass\"] \\\n",
    "        = \"BC2\"\n",
    "    # BC3 - tunnel inf rock\n",
    "    def bc3(self):\n",
    "        TunnExcvDF.loc[(TunnExcvDF[\"ExcavationType\"] == \"TBM\") & \\\n",
    "        (TunnExcvDF[\"RockSurface\"] >= TunnExcvDF[\"Elevation\"] +tunn_h/2.0 +1.5),\"BoreClass\"] \\\n",
    "        = \"BC3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# create alignment_df (dataframe) from .csv\n",
    "# Important: Before the df is created the data should be checked.\n",
    "#   E.g. make sure that it does not contain trailing blank lines and that duplicate lines are deleted.\n",
    "# result: alignment_df\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "alignment_df = pd.read_csv(AlignmentData)\n",
    "#delete row if only NA are present in row\n",
    "alignment_df = alignment_df.dropna(how = \"all\")\n",
    "# round alignment_df to three decimals\n",
    "alignment_df = alignment_df.round(decimals=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Point</th>\n",
       "      <th>Type</th>\n",
       "      <th>Station</th>\n",
       "      <th>Northing</th>\n",
       "      <th>Easting</th>\n",
       "      <th>Elevation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>W100:2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>103+767.802</td>\n",
       "      <td>1268897.460</td>\n",
       "      <td>2610615.655</td>\n",
       "      <td>241.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BC1:3</td>\n",
       "      <td>BC</td>\n",
       "      <td>103+917.325</td>\n",
       "      <td>1268894.622</td>\n",
       "      <td>2610765.151</td>\n",
       "      <td>249.226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>W100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104+071.807</td>\n",
       "      <td>1268891.690</td>\n",
       "      <td>2610919.605</td>\n",
       "      <td>254.564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>104+073</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104+073</td>\n",
       "      <td>1268891.667</td>\n",
       "      <td>2610920.798</td>\n",
       "      <td>254.587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>104+076</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104+076</td>\n",
       "      <td>1268891.609</td>\n",
       "      <td>2610923.797</td>\n",
       "      <td>254.642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>104+079</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104+079</td>\n",
       "      <td>1268891.552</td>\n",
       "      <td>2610926.797</td>\n",
       "      <td>254.696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>104+082</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104+082</td>\n",
       "      <td>1268891.494</td>\n",
       "      <td>2610929.796</td>\n",
       "      <td>254.748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>104+085</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104+085</td>\n",
       "      <td>1268891.436</td>\n",
       "      <td>2610932.796</td>\n",
       "      <td>254.798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>104+088</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104+088</td>\n",
       "      <td>1268891.378</td>\n",
       "      <td>2610935.795</td>\n",
       "      <td>254.847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>104+091</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104+091</td>\n",
       "      <td>1268891.321</td>\n",
       "      <td>2610938.794</td>\n",
       "      <td>254.894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>104+094</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104+094</td>\n",
       "      <td>1268891.263</td>\n",
       "      <td>2610941.794</td>\n",
       "      <td>254.939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>104+097</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104+097</td>\n",
       "      <td>1268891.205</td>\n",
       "      <td>2610944.793</td>\n",
       "      <td>254.982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>104+100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104+100</td>\n",
       "      <td>1268891.148</td>\n",
       "      <td>2610947.793</td>\n",
       "      <td>255.023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>104+103</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104+103</td>\n",
       "      <td>1268891.090</td>\n",
       "      <td>2610950.792</td>\n",
       "      <td>255.062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>104+106</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104+106</td>\n",
       "      <td>1268891.032</td>\n",
       "      <td>2610953.792</td>\n",
       "      <td>255.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>104+109</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104+109</td>\n",
       "      <td>1268890.974</td>\n",
       "      <td>2610956.791</td>\n",
       "      <td>255.136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>104+112</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104+112</td>\n",
       "      <td>1268890.917</td>\n",
       "      <td>2610959.791</td>\n",
       "      <td>255.170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>104+115</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104+115</td>\n",
       "      <td>1268890.859</td>\n",
       "      <td>2610962.790</td>\n",
       "      <td>255.202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>104+118</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104+118</td>\n",
       "      <td>1268890.801</td>\n",
       "      <td>2610965.789</td>\n",
       "      <td>255.233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>104+121</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104+121</td>\n",
       "      <td>1268890.743</td>\n",
       "      <td>2610968.789</td>\n",
       "      <td>255.261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>104+124</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104+124</td>\n",
       "      <td>1268890.686</td>\n",
       "      <td>2610971.788</td>\n",
       "      <td>255.288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>104+127</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104+127</td>\n",
       "      <td>1268890.628</td>\n",
       "      <td>2610974.788</td>\n",
       "      <td>255.313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>104+130</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104+130</td>\n",
       "      <td>1268890.570</td>\n",
       "      <td>2610977.787</td>\n",
       "      <td>255.337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>104+133</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104+133</td>\n",
       "      <td>1268890.512</td>\n",
       "      <td>2610980.787</td>\n",
       "      <td>255.358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>104+136</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104+136</td>\n",
       "      <td>1268890.455</td>\n",
       "      <td>2610983.786</td>\n",
       "      <td>255.378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>104+139</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104+139</td>\n",
       "      <td>1268890.397</td>\n",
       "      <td>2610986.786</td>\n",
       "      <td>255.396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>104+142</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104+142</td>\n",
       "      <td>1268890.339</td>\n",
       "      <td>2610989.785</td>\n",
       "      <td>255.412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>104+145</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104+145</td>\n",
       "      <td>1268890.282</td>\n",
       "      <td>2610992.784</td>\n",
       "      <td>255.426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>104+148</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104+148</td>\n",
       "      <td>1268890.224</td>\n",
       "      <td>2610995.784</td>\n",
       "      <td>255.439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>104+151</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104+151</td>\n",
       "      <td>1268890.166</td>\n",
       "      <td>2610998.783</td>\n",
       "      <td>255.449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3717</th>\n",
       "      <td>109+545</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109+545</td>\n",
       "      <td>1265939.767</td>\n",
       "      <td>2614548.307</td>\n",
       "      <td>266.292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3719</th>\n",
       "      <td>109+548</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109+548</td>\n",
       "      <td>1265936.937</td>\n",
       "      <td>2614547.310</td>\n",
       "      <td>266.265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3721</th>\n",
       "      <td>109+551</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109+551</td>\n",
       "      <td>1265934.107</td>\n",
       "      <td>2614546.314</td>\n",
       "      <td>266.235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3723</th>\n",
       "      <td>109+554</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109+554</td>\n",
       "      <td>1265931.278</td>\n",
       "      <td>2614545.317</td>\n",
       "      <td>266.203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3725</th>\n",
       "      <td>109+557</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109+557</td>\n",
       "      <td>1265928.448</td>\n",
       "      <td>2614544.321</td>\n",
       "      <td>266.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3727</th>\n",
       "      <td>109+560</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109+560</td>\n",
       "      <td>1265925.618</td>\n",
       "      <td>2614543.324</td>\n",
       "      <td>266.128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3729</th>\n",
       "      <td>109+563</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109+563</td>\n",
       "      <td>1265922.789</td>\n",
       "      <td>2614542.328</td>\n",
       "      <td>266.086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3731</th>\n",
       "      <td>109+566</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109+566</td>\n",
       "      <td>1265919.959</td>\n",
       "      <td>2614541.331</td>\n",
       "      <td>266.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3733</th>\n",
       "      <td>109+569</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109+569</td>\n",
       "      <td>1265917.129</td>\n",
       "      <td>2614540.335</td>\n",
       "      <td>265.993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3735</th>\n",
       "      <td>109+572</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109+572</td>\n",
       "      <td>1265914.300</td>\n",
       "      <td>2614539.338</td>\n",
       "      <td>265.942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3737</th>\n",
       "      <td>109+575</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109+575</td>\n",
       "      <td>1265911.470</td>\n",
       "      <td>2614538.342</td>\n",
       "      <td>265.888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3739</th>\n",
       "      <td>109+578</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109+578</td>\n",
       "      <td>1265908.640</td>\n",
       "      <td>2614537.345</td>\n",
       "      <td>265.831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3741</th>\n",
       "      <td>109+581</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109+581</td>\n",
       "      <td>1265905.811</td>\n",
       "      <td>2614536.348</td>\n",
       "      <td>265.771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3743</th>\n",
       "      <td>109+584</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109+584</td>\n",
       "      <td>1265902.981</td>\n",
       "      <td>2614535.352</td>\n",
       "      <td>265.708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3745</th>\n",
       "      <td>109+587</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109+587</td>\n",
       "      <td>1265900.152</td>\n",
       "      <td>2614534.355</td>\n",
       "      <td>265.642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3747</th>\n",
       "      <td>109+590</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109+590</td>\n",
       "      <td>1265897.322</td>\n",
       "      <td>2614533.359</td>\n",
       "      <td>265.573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3749</th>\n",
       "      <td>109+593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109+593</td>\n",
       "      <td>1265894.492</td>\n",
       "      <td>2614532.362</td>\n",
       "      <td>265.501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3751</th>\n",
       "      <td>109+596</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109+596</td>\n",
       "      <td>1265891.663</td>\n",
       "      <td>2614531.366</td>\n",
       "      <td>265.426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3753</th>\n",
       "      <td>109+599</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109+599</td>\n",
       "      <td>1265888.833</td>\n",
       "      <td>2614530.369</td>\n",
       "      <td>265.348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3755</th>\n",
       "      <td>109+602</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109+602</td>\n",
       "      <td>1265886.003</td>\n",
       "      <td>2614529.372</td>\n",
       "      <td>265.267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3757</th>\n",
       "      <td>109+605</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109+605</td>\n",
       "      <td>1265883.174</td>\n",
       "      <td>2614528.376</td>\n",
       "      <td>265.183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3759</th>\n",
       "      <td>109+608</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109+608</td>\n",
       "      <td>1265880.344</td>\n",
       "      <td>2614527.379</td>\n",
       "      <td>265.096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3761</th>\n",
       "      <td>109+611</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109+611</td>\n",
       "      <td>1265877.514</td>\n",
       "      <td>2614526.383</td>\n",
       "      <td>265.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3763</th>\n",
       "      <td>109+614</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109+614</td>\n",
       "      <td>1265874.685</td>\n",
       "      <td>2614525.386</td>\n",
       "      <td>264.912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3765</th>\n",
       "      <td>109+617</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109+617</td>\n",
       "      <td>1265871.855</td>\n",
       "      <td>2614524.390</td>\n",
       "      <td>264.816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3767</th>\n",
       "      <td>109+620</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109+620</td>\n",
       "      <td>1265869.026</td>\n",
       "      <td>2614523.393</td>\n",
       "      <td>264.717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3769</th>\n",
       "      <td>109+623</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109+623</td>\n",
       "      <td>1265866.196</td>\n",
       "      <td>2614522.397</td>\n",
       "      <td>264.615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3771</th>\n",
       "      <td>W133</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109+624.056</td>\n",
       "      <td>1265865.200</td>\n",
       "      <td>2614522.046</td>\n",
       "      <td>264.579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3773</th>\n",
       "      <td>EC6:5</td>\n",
       "      <td>EC</td>\n",
       "      <td>109+669.433</td>\n",
       "      <td>1265822.400</td>\n",
       "      <td>2614506.972</td>\n",
       "      <td>262.653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3775</th>\n",
       "      <td>W149</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109+724.056</td>\n",
       "      <td>1265770.878</td>\n",
       "      <td>2614488.828</td>\n",
       "      <td>259.922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1888 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Point Type      Station     Northing      Easting  Elevation\n",
       "1      W100:2  NaN  103+767.802  1268897.460  2610615.655    241.750\n",
       "3       BC1:3   BC  103+917.325  1268894.622  2610765.151    249.226\n",
       "5        W100  NaN  104+071.807  1268891.690  2610919.605    254.564\n",
       "7     104+073  NaN      104+073  1268891.667  2610920.798    254.587\n",
       "9     104+076  NaN      104+076  1268891.609  2610923.797    254.642\n",
       "11    104+079  NaN      104+079  1268891.552  2610926.797    254.696\n",
       "13    104+082  NaN      104+082  1268891.494  2610929.796    254.748\n",
       "15    104+085  NaN      104+085  1268891.436  2610932.796    254.798\n",
       "17    104+088  NaN      104+088  1268891.378  2610935.795    254.847\n",
       "19    104+091  NaN      104+091  1268891.321  2610938.794    254.894\n",
       "21    104+094  NaN      104+094  1268891.263  2610941.794    254.939\n",
       "23    104+097  NaN      104+097  1268891.205  2610944.793    254.982\n",
       "25    104+100  NaN      104+100  1268891.148  2610947.793    255.023\n",
       "27    104+103  NaN      104+103  1268891.090  2610950.792    255.062\n",
       "29    104+106  NaN      104+106  1268891.032  2610953.792    255.100\n",
       "31    104+109  NaN      104+109  1268890.974  2610956.791    255.136\n",
       "33    104+112  NaN      104+112  1268890.917  2610959.791    255.170\n",
       "35    104+115  NaN      104+115  1268890.859  2610962.790    255.202\n",
       "37    104+118  NaN      104+118  1268890.801  2610965.789    255.233\n",
       "39    104+121  NaN      104+121  1268890.743  2610968.789    255.261\n",
       "41    104+124  NaN      104+124  1268890.686  2610971.788    255.288\n",
       "43    104+127  NaN      104+127  1268890.628  2610974.788    255.313\n",
       "45    104+130  NaN      104+130  1268890.570  2610977.787    255.337\n",
       "47    104+133  NaN      104+133  1268890.512  2610980.787    255.358\n",
       "49    104+136  NaN      104+136  1268890.455  2610983.786    255.378\n",
       "51    104+139  NaN      104+139  1268890.397  2610986.786    255.396\n",
       "53    104+142  NaN      104+142  1268890.339  2610989.785    255.412\n",
       "55    104+145  NaN      104+145  1268890.282  2610992.784    255.426\n",
       "57    104+148  NaN      104+148  1268890.224  2610995.784    255.439\n",
       "59    104+151  NaN      104+151  1268890.166  2610998.783    255.449\n",
       "...       ...  ...          ...          ...          ...        ...\n",
       "3717  109+545  NaN      109+545  1265939.767  2614548.307    266.292\n",
       "3719  109+548  NaN      109+548  1265936.937  2614547.310    266.265\n",
       "3721  109+551  NaN      109+551  1265934.107  2614546.314    266.235\n",
       "3723  109+554  NaN      109+554  1265931.278  2614545.317    266.203\n",
       "3725  109+557  NaN      109+557  1265928.448  2614544.321    266.167\n",
       "3727  109+560  NaN      109+560  1265925.618  2614543.324    266.128\n",
       "3729  109+563  NaN      109+563  1265922.789  2614542.328    266.086\n",
       "3731  109+566  NaN      109+566  1265919.959  2614541.331    266.041\n",
       "3733  109+569  NaN      109+569  1265917.129  2614540.335    265.993\n",
       "3735  109+572  NaN      109+572  1265914.300  2614539.338    265.942\n",
       "3737  109+575  NaN      109+575  1265911.470  2614538.342    265.888\n",
       "3739  109+578  NaN      109+578  1265908.640  2614537.345    265.831\n",
       "3741  109+581  NaN      109+581  1265905.811  2614536.348    265.771\n",
       "3743  109+584  NaN      109+584  1265902.981  2614535.352    265.708\n",
       "3745  109+587  NaN      109+587  1265900.152  2614534.355    265.642\n",
       "3747  109+590  NaN      109+590  1265897.322  2614533.359    265.573\n",
       "3749  109+593  NaN      109+593  1265894.492  2614532.362    265.501\n",
       "3751  109+596  NaN      109+596  1265891.663  2614531.366    265.426\n",
       "3753  109+599  NaN      109+599  1265888.833  2614530.369    265.348\n",
       "3755  109+602  NaN      109+602  1265886.003  2614529.372    265.267\n",
       "3757  109+605  NaN      109+605  1265883.174  2614528.376    265.183\n",
       "3759  109+608  NaN      109+608  1265880.344  2614527.379    265.096\n",
       "3761  109+611  NaN      109+611  1265877.514  2614526.383    265.005\n",
       "3763  109+614  NaN      109+614  1265874.685  2614525.386    264.912\n",
       "3765  109+617  NaN      109+617  1265871.855  2614524.390    264.816\n",
       "3767  109+620  NaN      109+620  1265869.026  2614523.393    264.717\n",
       "3769  109+623  NaN      109+623  1265866.196  2614522.397    264.615\n",
       "3771     W133  NaN  109+624.056  1265865.200  2614522.046    264.579\n",
       "3773    EC6:5   EC  109+669.433  1265822.400  2614506.972    262.653\n",
       "3775     W149  NaN  109+724.056  1265770.878  2614488.828    259.922\n",
       "\n",
       "[1888 rows x 6 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alignment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# create layout_df from .csv\n",
    "# result: layout_df\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "layout_df = pd.read_csv(LayoutData)\n",
    "# round layout_df to three decimals\n",
    "layout_df = layout_df.round(decimals=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#layout_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating df header StationReal\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# convert alignment_df[\"Station\"] => alignment_df[\"StationReal\"] and similar for layout_df\n",
    "# result: alignment_Station_list, layout_Station_list\n",
    "#         alignment_df[\"StationReal\"], layout_df[\"StationReal\"]\n",
    "print \"creating df header StationReal\"\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "alignment_Station_list = alignment_df[\"Station\"].tolist()\n",
    "    # check: len(alignment_Station_list)\n",
    "\n",
    "alignment_df[\"StationReal\"] = np.nan\n",
    "\n",
    "for n in range(0, len(alignment_Station_list)):\n",
    "    station_sel = alignment_df.iloc[n][\"Station\"]\n",
    "    station_real_sel = float(station_sel.replace(\"+\",\"\"))\n",
    "    alignment_df.iloc[n, alignment_df.columns.get_loc(\"StationReal\")] = station_real_sel\n",
    "    # alignment_df.columns.get_loc(\"StationReal\") = 5\n",
    "\n",
    "# layout_df[\"Station\"] => layout_df[\"StationReal\"] \n",
    "layout_Station_list = layout_df[\"Station\"].tolist()\n",
    "    # check: len(layout_Station_list)\n",
    "\n",
    "layout_df[\"StationReal\"] = np.nan\n",
    "\n",
    "for n in range(0, len(layout_Station_list)):\n",
    "    station_sel = layout_df.iloc[n][\"Station\"]\n",
    "    station_real_sel = float(station_sel.replace(\"+\",\"\"))\n",
    "    layout_df.iloc[n, layout_df.columns.get_loc(\"StationReal\")] \\\n",
    "        = station_real_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding Stations\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# check if every layout_df[\"StationReal\"] exists in alignment_df[\"StationReal\"]\n",
    "#   If it does not exist, create a new Station in alignment_df\n",
    "# result: alignment_StationReal_list, layout_StationReal_list\n",
    "#         alignment_df with added Stations\n",
    "print \"adding Stations\"\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "alignment_StationReal_list = alignment_df[\"StationReal\"].tolist()\n",
    "layout_StationReal_list = layout_df[\"StationReal\"].tolist()\n",
    "\n",
    "# within this vicinity of a station no new station will be created\n",
    "vicinity = 0.1\n",
    "\n",
    "# define new variables\n",
    "easting_newpoint = []\n",
    "northing_newpoint = []\n",
    "elevation_newpoint = []\n",
    "station = []\n",
    "station_real = []\n",
    "\n",
    "# Loop through stations\n",
    "for n in layout_StationReal_list:\n",
    "    if n in alignment_StationReal_list:\n",
    "        pass\n",
    "    else:\n",
    "        neighbour1_StationReal = max([i for i in alignment_StationReal_list if i < n]) \n",
    "        neighbour2_StationReal = min([i for i in alignment_StationReal_list if i > n])       \n",
    "        ####if n < neighbour1_StationReal + vicinity or n > neighbour2_StationReal + vicinity:      KLK: check\n",
    "        if n < neighbour1_StationReal + vicinity or n > neighbour2_StationReal - vicinity:\n",
    "            pass\n",
    "        else: \n",
    "            neighbour1 = alignment_df.loc[alignment_df[\"StationReal\"]\n",
    "                                          == neighbour1_StationReal,]\n",
    "            neighbour2 = alignment_df.loc[alignment_df[\"StationReal\"]\n",
    "                                          == neighbour2_StationReal,]\n",
    "            ####delta_x_neighbour1_2 = abs(neighbour2.Easting.tolist()[0] -neighbour1.Easting.tolist()[0])\n",
    "            ####delta_y_neighbour1_2 = abs(neighbour2.Northing.tolist()[0] -neighbour1.Northing.tolist()[0])\n",
    "            ####                                                                                  KLK: check\n",
    "            delta_x_neighbour1_2 = neighbour2.Easting.tolist()[0] -neighbour1.Easting.tolist()[0] #delta x\n",
    "            delta_y_neighbour1_2 = neighbour2.Northing.tolist()[0] -neighbour1.Northing.tolist()[0] #delta y\n",
    "            delta_z_neighbour1_2 = neighbour2.Elevation.tolist()[0] -neighbour1.Elevation.tolist()[0] #delta z\n",
    "            length_neighbour1_2 = (delta_x_neighbour1_2**2 +delta_y_neighbour1_2**2)**(0.5) # L\n",
    "            length_neighbour1_newpoint = n- neighbour1.StationReal.tolist()[0]\n",
    "            ####easting_newpoint_sel = neighbour2.Easting.tolist()[0] \\\n",
    "            ####                     +((delta_y_neighbour1_2*length_neighbour1_newpoint)/length_neighbour1_2)\n",
    "            ####northing_newpoint_sel = neighbour2.Northing.tolist()[0] \\\n",
    "            ####                      +((delta_x_neighbour1_2*length_neighbour1_newpoint)/length_neighbour1_2)\n",
    "            ####elevation_newpoint_sel = neighbour2.Elevation.tolist()[0] \\\n",
    "            ####                       +((delta_x_neighbour1_2*length_neighbour1_newpoint)/length_neighbour1_2)\n",
    "            ####                                                                                  KLK: check\n",
    "            easting_newpoint_sel = neighbour1.Easting.tolist()[0] \\\n",
    "                                 +((delta_x_neighbour1_2*length_neighbour1_newpoint)/length_neighbour1_2)\n",
    "            northing_newpoint_sel = neighbour1.Northing.tolist()[0] \\\n",
    "                                  +((delta_y_neighbour1_2*length_neighbour1_newpoint)/length_neighbour1_2)\n",
    "            elevation_newpoint_sel = neighbour1.Elevation.tolist()[0] \\\n",
    "                                   +((delta_z_neighbour1_2*length_neighbour1_newpoint)/length_neighbour1_2)\n",
    "            easting_newpoint.append(easting_newpoint_sel)\n",
    "            northing_newpoint.append(northing_newpoint_sel)\n",
    "            elevation_newpoint.append(elevation_newpoint_sel)\n",
    "            station_real.append(n)\n",
    "            print \"    \", neighbour1\n",
    "            print \"    \", neighbour2\n",
    "            print \"    station_real\", n\n",
    "            print \"    easting_newpoint_sel\", easting_newpoint_sel\n",
    "            print \"    northing_newpoint_sel\", northing_newpoint_sel\n",
    "            print \"    elevation_newpoint_sel\", elevation_newpoint_sel\n",
    "            # this procedure must be tested for all combinations of ascending/descending\n",
    "            #   Northing, Easting and Elevation --> should be OK\n",
    "            #   for descending Stationing --> needs fixing                                          JK ToDo\n",
    "            Station_sel = layout_df.loc[layout_df['StationReal']\n",
    "                                                  == n, 'Station'] \n",
    "            station.append(Station_sel.tolist()[0])\n",
    "           \n",
    "newStation_df = pd.DataFrame({\"Easting\": easting_newpoint, \"Northing\": northing_newpoint,\n",
    "                              \"Elevation\": elevation_newpoint, \"StationReal\": station_real,\n",
    "                              \"Station\": station})\n",
    "    # check len(alignment_df)\n",
    "    # check len(newStation_df)\n",
    "\n",
    "# Contatenate alignment_df with newStation_df\n",
    "# result: alignment_df\n",
    "frames = [alignment_df, newStation_df]\n",
    "alignment_df = pd.concat(frames)\n",
    "    # check: len(alignment_df)\n",
    "    # check: alignment_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# create Alignment_spatial from alignment_df\n",
    "# result: Alignment_SHP\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "Alignment_spatial_points = [sp.geometry.Point(row['Easting'], row['Northing'])\n",
    "                            for key, row in alignment_df.iterrows()]\n",
    "Alignment_crs = {'init': 'epsg:2056'}  #define crs\n",
    "Alignment_spatial = gpd.GeoDataFrame(alignment_df, geometry=Alignment_spatial_points, crs = Alignment_crs)\n",
    "Alignment_spatial.to_file(Alignment_SHP, driver='ESRI Shapefile') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get raster values\n",
      "Warning: Not all input layers use the same CRS.\n",
      "This can cause unexpected results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'output': 'data/out/Westroehre.RockSurface.R2.csv'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# use grass functions to get raster values for points along tunnel axis and write to .csv files\n",
    "# result: Alignment_DTM, Alignment_RockSurface\n",
    "print \"get raster values\"\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Alignment_DTM\n",
    "processing.runalg(\"grass7:r.what.points\",DTM,Alignment_SHP,\n",
    "                  \"NA\",\",\",500,True,False,False,False,False,\n",
    "                  \"2603510.0,2624270.0,1260650.0,1274890.0\",-1,0.0001,Alignment_DTM)\n",
    "# Alignment_RockSurface                  \n",
    "processing.runalg(\"grass7:r.what.points\",RockSurface,Alignment_SHP,\n",
    "                  \"NA\",\",\",500, True,False,False,False,False,\n",
    "                  \"2603510.0,2624270.0,1260650.0,1274890.0\",-1,0.0001,Alignment_RockSurface)\n",
    "## warning: Not all input layers use the same CRS -> data seems OK\n",
    "    # check:  Alginemnt_spatial.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# create df's\n",
    "# result: Alignment_DTM_df, Alignment_RockSurface_df\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "Alignment_DTM_df  = pd.read_csv(Alignment_DTM)\n",
    "Alignment_RockSurface_df  = pd.read_csv(Alignment_RockSurface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Alignment_DTM_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# prepare for join of grass results using pandas\n",
    "# result: alignment_df, Alignment_DTM_df_sel, Alignment_RockSurface_df_sel\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# prepare alignment_df\n",
    "    # check:  alignment_df.head()\n",
    "alignment_df = alignment_df.loc[:,[\"Station\",\"Easting\", \"Northing\", \"Elevation\", \"StationReal\"]]\n",
    "    # check:  alignment_df.head()\n",
    "\n",
    "# prepare Alignment_DTM_df_sel\n",
    "    # check:  Alignment_DTM_df.head()\n",
    "Alignment_DTM_df_coleqtmp = [col for col in Alignment_DTM_df.columns if 'tmp' in col]\n",
    "if len(Alignment_DTM_df_coleqtmp) != 1:\n",
    "    print \"Extraction of DTM col=tmp did not work properly. Please check\"\n",
    "    exit()\n",
    "Alignment_DTM_df_rename = Alignment_DTM_df.rename(\n",
    "    columns= {Alignment_DTM_df_coleqtmp[0]: \"DTM\"})\n",
    "Alignment_DTM_df_sel = Alignment_DTM_df_rename.loc[:,[\"easting\", \"northing\", \"DTM\"]]\n",
    "    # check:  Alignment_RockSurface_df.head()\n",
    "\n",
    "# prepare Alignment_RockSurface_df_coleqtmp\n",
    "Alignment_RockSurface_df_coleqtmp = [col for col in Alignment_RockSurface_df.columns if 'tmp' in col]\n",
    "if len(Alignment_RockSurface_df_coleqtmp) != 1:\n",
    "    print \"Extraction of RockSurface_csv_coleqtmp col=tmp did not work properly. Please check\"\n",
    "    exit()\n",
    "Alignment_RockSurface_df_rename = Alignment_RockSurface_df.rename(\n",
    "    columns= {Alignment_RockSurface_df_coleqtmp[0]: \"RockSurface\"})\n",
    "Alignment_RockSurface_df_sel = Alignment_RockSurface_df_rename.loc[\n",
    "    :,[\"easting\", \"northing\", \"RockSurface\"]]  \n",
    "    # check:  Alignment_RockSurface_df_sel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge_final\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'alignment_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d3487d42e42c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# ----------------------------------------------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0malignment_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malignment_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecimals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mAlignment_DTM_df_sel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAlignment_DTM_df_sel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecimals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mAlignment_RockSurface_df_sel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAlignment_RockSurface_df_sel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecimals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'alignment_df' is not defined"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# join grass results using Panda\n",
    "#    merge handles floats as keys inconsistently, round df's to three decimals before merge \n",
    "# result: merge_final\n",
    "print 'merge_final'\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "alignment_df = alignment_df.round(decimals=3)\n",
    "Alignment_DTM_df_sel = Alignment_DTM_df_sel.round(decimals=3)\n",
    "Alignment_RockSurface_df_sel = Alignment_RockSurface_df_sel.round(decimals=3)\n",
    "\n",
    "# merge DTM to Alignment\n",
    "merge_Alignment_DTM= pd.merge(left= alignment_df, right = Alignment_DTM_df_sel, \n",
    "                 left_on = [\"Easting\",\"Northing\"], \n",
    "                 right_on = [\"easting\",\"northing\"], how = \"left\")\n",
    "\n",
    "# merge RockSurface to Alignment_DTM\n",
    "merge_final = pd.merge(merge_Alignment_DTM, Alignment_RockSurface_df_sel, \n",
    "                 left_on = [\"Easting\",\"Northing\"], \n",
    "                 right_on = [\"easting\",\"northing\"])\n",
    "    # check:  merge_final.head()\n",
    "    # check:  merge_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning up merge\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# clean up merge_final\n",
    "# result: TunnExcvDF\n",
    "print 'cleaning up merge'\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "TunnExcvDF = merge_final.loc[:,[\"Station\",\"Easting\", \"Northing\", \"Elevation\", \"DTM\", \"RockSurface\",\n",
    "                               \"StationReal\"]]\n",
    "    # check:  TunnExcvDF.head()\n",
    "    # check:  list(TunnExcvDF)\n",
    "# sort by Station\n",
    "#TunnExcvDF = TunnExcvDF.sort(['StationReal'], ascending=[1])  #sort depreacted\n",
    "TunnExcvDF = TunnExcvDF.sort_values(['StationReal'], ascending=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# calculate difference height rocksurface and tunnel axis\n",
    "# result: TunnExcvDF['RockCover']\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "TunnExcvDF['RockCover'] = TunnExcvDF.RockSurface - TunnExcvDF.Elevation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on WBS etc\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# assign WBS, WorkType, Excavation Type, Profile Type, Section Area from TunnelLayoutDarta\n",
    "# result: TunnExcvDF[\"WBScode\"], TunnExcvDF[\"WorkType\"], TunnExcvDF[\"ExcavationType\"], TunnExcvDF[\"ProfileType\"]\n",
    "#         TunnExcvDF[\"SectionArea\"], TunnExcvDF[\"Description\"]\n",
    "print \"working on WBS etc\"\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "TunnExcvDF[\"WBScode\"] = np.nan\n",
    "TunnExcvDF[\"WorkType\"] = np.nan\n",
    "TunnExcvDF[\"ExcavationType\"] = np.nan\n",
    "TunnExcvDF[\"ProfileType\"] = np.nan\n",
    "TunnExcvDF[\"SectionArea\"] = np.nan\n",
    "TunnExcvDF[\"Description\"] = np.nan\n",
    "TunnExcvDF[\"Unit\"] = volume_unit\n",
    "\n",
    "for n in range(0, len(layout_StationReal_list)):\n",
    "    nn = n+1 \n",
    "    if n == len(layout_StationReal_list) -1:\n",
    "        layout_StationReal_list.append(1e12)\n",
    "    TunnExcvDF.loc[(TunnExcvDF[\"StationReal\"] >= layout_StationReal_list[n])\n",
    "        & (TunnExcvDF[\"StationReal\"] < layout_StationReal_list[nn]), \"WBScode\"] \\\n",
    "        = layout_df[\"WBScode\"].tolist()[n]\n",
    "    TunnExcvDF.loc[(TunnExcvDF[\"StationReal\"] >= layout_StationReal_list[n])\n",
    "        & (TunnExcvDF[\"StationReal\"] < layout_StationReal_list[nn]), \"WorkType\"] \\\n",
    "        = layout_df[\"WorkType\"].tolist()[n]\n",
    "    TunnExcvDF.loc[(TunnExcvDF[\"StationReal\"] >= layout_StationReal_list[n])\n",
    "        & (TunnExcvDF[\"StationReal\"] < layout_StationReal_list[nn]), \"ExcavationType\"] \\\n",
    "        = layout_df[\"ExcavationType\"].tolist()[n]\n",
    "    TunnExcvDF.loc[(TunnExcvDF[\"StationReal\"] >= layout_StationReal_list[n])\n",
    "        & (TunnExcvDF[\"StationReal\"] < layout_StationReal_list[nn]), \"ProfileType\"] \\\n",
    "        = layout_df[\"ProfileType\"].tolist()[n]\n",
    "    TunnExcvDF.loc[(TunnExcvDF[\"StationReal\"] >= layout_StationReal_list[n])\n",
    "        & (TunnExcvDF[\"StationReal\"] < layout_StationReal_list[nn]), \"SectionArea\"] \\\n",
    "        = layout_df[\"SectionArea\"].tolist()[n]\n",
    "    TunnExcvDF.loc[(TunnExcvDF[\"StationReal\"] >= layout_StationReal_list[n])\n",
    "        & (TunnExcvDF[\"StationReal\"] < layout_StationReal_list[nn]), \"Description\"] \\\n",
    "        = layout_df[\"Description\"].tolist()[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating BoreClass, SupportClass and DisposalClass\n",
      "BC3    1273\n",
      "BC2      93\n",
      "Name: BoreClass, dtype: int64\n",
      "TBM    1366\n",
      "MUL      15\n",
      "Name: ExcavationType, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# calculate \"BoreClass\", \"SupportClass\" and \"DisposalClass\"\n",
    "# result: TunnExcvDF[\"BoreClass\"], TunnExcvDF[\"SupportClass\"], TunnExcvDF[\"DisposalClass\"]\n",
    "print 'calculating BoreClass, SupportClass and DisposalClass'\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "TunnExcvDF[\"BoreClass\"]= np.nan\n",
    "TunnExcvDF[\"SupportClass\"]= np.nan\n",
    "TunnExcvDF[\"DisposalClass\"]= np.nan\n",
    "\n",
    "# instantiate an instance of BoreClass\n",
    "bore_class=BoreClass()\n",
    "# call bore_class methods for BC1, BC2, BC3\n",
    "bore_class.bc1()\n",
    "bore_class.bc2()\n",
    "bore_class.bc3()\n",
    "print TunnExcvDF[\"BoreClass\"].value_counts()  # equals 805+188+60 for Ostroehre\n",
    "print TunnExcvDF[\"ExcavationType\"].value_counts() \n",
    "\n",
    "# Support Class                                                             # JK ToDo: define SC's as Class\n",
    "#  SCT\n",
    "TunnExcvDF.loc[(TunnExcvDF[\"ExcavationType\"] == \"TBM\"), \\\n",
    "    \"SupportClass\"] = \"SCT\"\n",
    "#  SC5\n",
    "TunnExcvDF.loc[(TunnExcvDF[\"ExcavationType\"] == \"MUL\"), \\\n",
    "    \"SupportClass\"] = \"SC5\"\n",
    "# check: TunnExcvDF[\"SupportClass\"].value_counts()\n",
    "# check: TunnExcvDF[\"ExcavationType\"].value_counts() \n",
    "\n",
    "# Disposal Class                                                            # JK ToDo: define MC's as Class\n",
    "#  MC5\n",
    "TunnExcvDF.loc[(TunnExcvDF[\"BoreClass\"] ==  \"BC1\") | (TunnExcvDF[\"BoreClass\"] == \"BC2\"), \\\n",
    "    \"DisposalClass\"] = \"MC5\"\n",
    "#  MC3\n",
    "TunnExcvDF.loc[(TunnExcvDF[\"BoreClass\"] ==  \"BC3\"), \\\n",
    "    \"DisposalClass\"] = \"MC3\"\n",
    "#  MC2\n",
    "TunnExcvDF.loc[(TunnExcvDF[\"ExcavationType\"] == \"MUL\"), \\\n",
    "    \"DisposalClass\"] = \"MC2\"\n",
    "# check: TunnExcvDF[\"DisposalClass\"].value_counts()\n",
    "# check: TunnExcvDF[\"ExcavationType\"].value_counts() # 805+248\n",
    "# check:\n",
    "#     print TunnExcvDF.loc[:,[\"Station\",\"ExcavationType\",\"BoreClass\",\"SupportClass\",\"DisposalClass\"]].to_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calcuating excavation volume\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# calculate excavation volume of tunnel between two axis points\n",
    "print 'calcuating excavation volume'\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# initialize interval length (StationInterval field)\n",
    "TunnExcvDF[\"StationInterval\"] = np.nan\n",
    "TunnExcvDF[\"ExcavationVolume\"] = np.nan\n",
    "\n",
    "# Calculate \"StationInterval\", \"Area1_mean_dist\" and \"Area2_mean_dist\"\n",
    "n = 0\n",
    "\n",
    "# use .iat instead of .iloc to return scalar values (*1000 faster)\n",
    "# LayoutData must show missing data as NaN (None is read as string value)\n",
    "for i in range(len(TunnExcvDF.index) -1):\n",
    "    nn= n+1\n",
    "    TunnExcvDF[\"StationInterval\"].iat[n] = ((TunnExcvDF[\"Easting\"].iat[nn] -TunnExcvDF[\"Easting\"].iat[n])**2 \n",
    "        +(TunnExcvDF[\"Northing\"].iat[nn] -TunnExcvDF[\"Northing\"].iat[n])**2 \n",
    "        +(TunnExcvDF[\"Elevation\"].iat[nn] -TunnExcvDF[\"Elevation\"].iat[n])**2 )**(0.5)\n",
    "    TunnExcvDF[\"ExcavationVolume\"].iat[n] = TunnExcvDF[\"SectionArea\"].iat[n] * TunnExcvDF[\"StationInterval\"].iat[n]\n",
    "    n = n+1\n",
    "# check:\n",
    "#    print TunnExcvDF.loc[:,[\"Station\",\"ExcavationType\",\"StationInterval\",\"ExcavationVolume\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calcuating disposal volume\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# calculate disposal volume of tunnel between two axis points\n",
    "print 'calcuating disposal volume'\n",
    "# result: file TunnelExcavationData as .csv)\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# mv to beginning of file                                                        JK ToDo\n",
    "def disposal_volume(ExcavationVolume, DisposalClass):\n",
    "    #calculate Disposal Volumes based on Disposal Class\n",
    "    DisposalVolume=np.nan\n",
    "    if DisposalClass==\"MC2\":\n",
    "        DisposalVolume=1.3*ExcavationVolume\n",
    "    elif DisposalClass==\"MC3\":\n",
    "        DisposalVolume=1.5*ExcavationVolume \n",
    "    elif DisposalClass==\"MC5\":\n",
    "        DisposalVolume=1.3*ExcavationVolume\n",
    "    #else:\n",
    "        #print \"unknown disposal class\"\n",
    "    return DisposalVolume\n",
    "\n",
    "TunnExcvDF[\"DisposalVolume\"] = np.nan\n",
    "n = 0\n",
    "for i in range(len(TunnExcvDF.index) -1):\n",
    "    TunnExcvDF[\"DisposalVolume\"].iat[n] = (\n",
    "        disposal_volume(TunnExcvDF[\"ExcavationVolume\"].iat[n],TunnExcvDF[\"DisposalClass\"].iat[n]) )\n",
    "    n = n+1\n",
    "# check:\n",
    "#  print TunnExcvDF.loc[:,[\"Station\",\"DisposalType\",\"ExcavationVolume\",\"DisposalVolume\"]]\n",
    "\n",
    "TunnExcvDF.to_csv(TunnelExcavationData, sep=\",\", na_rep=\"NaN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating BoQ\n",
      "211a UEX MUL 104+583 104+604 SC5 2526.2240147201637 m3\n",
      "211a UEX MUL 104+583 104+604 MC2 3284.091219136213 m3\n",
      "211b UEX TBM 104+604 108+627 BC2 36286.54608848064 m3\n",
      "211b UEX TBM 104+604 108+627 BC3 497723.3856782965 m3\n",
      "211b UEX TBM 104+604 108+627 SCT 534009.9317667771 m3\n",
      "211b UEX TBM 104+604 108+627 MC5 47172.509915024835 m3\n",
      "211b UEX TBM 104+604 108+627 MC3 746585.0785174447 m3\n",
      "211c UEX MUL 108+627 108+648 SC5 2523.118569557004 m3\n",
      "211c UEX MUL 108+627 108+648 MC2 3280.054140424105 m3\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# create BoQ and write to file\n",
    "# results: BoQ_df and BoQ as .csv\n",
    "print 'creating BoQ'\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# replace print with write to file                                             ToDo JK\n",
    "\n",
    "# initialize a BoQ_list\n",
    "BoQ_list_headers= [\"WBS\",\"WorkType\",\"ExcavationType\",\"StationFrom\",\"StationTo\",\"PayItem\",\"Quantity\",\"Unit\"]\n",
    "BoQ_list_values=[]\n",
    "\n",
    "# find combinations of WBScode, ExcavationType and [BoreClass | Support Class | Disposal Class that exist\n",
    "# calculate excavation volume for each combination\n",
    "for i in TunnExcvDF[\"WBScode\"].unique():\n",
    "    for j in TunnExcvDF[\"ExcavationType\"].unique():\n",
    "        if ((TunnExcvDF[\"WBScode\"] == i)\n",
    "             & (TunnExcvDF[\"ExcavationType\"] == j)).any():\n",
    "            work_type = (TunnExcvDF.loc[\n",
    "                ((TunnExcvDF[\"WBScode\"] == i)\n",
    "                & (TunnExcvDF[\"ExcavationType\"] == j)),\"WorkType\"]).unique()[0]\n",
    "        for k in TunnExcvDF[\"BoreClass\"].unique():\n",
    "            # if DF record with i, j, k (as Bore Class) exists:\n",
    "            if ((TunnExcvDF[\"WBScode\"] == i)\n",
    "                & (TunnExcvDF[\"ExcavationType\"] == j)\n",
    "                & (TunnExcvDF[\"BoreClass\"] == k)).any():\n",
    "                start_station = min(TunnExcvDF.loc[\n",
    "                    ((TunnExcvDF[\"WBScode\"] == i)\n",
    "                     & (TunnExcvDF[\"ExcavationType\"] == j)),\"Station\"])\n",
    "                end_station = max(TunnExcvDF.loc[\n",
    "                    ((TunnExcvDF[\"WBScode\"] == i)\n",
    "                     & (TunnExcvDF[\"ExcavationType\"] == j)),\"Station\"])\n",
    "                #need 'Station +1' because we are going From: To: along alignment\n",
    "                #TunnExcvDF.loc[(TunnExcvDF[\"Station\"] == end_station),\"Station\"].values[0]    ToDo Note JK\n",
    "                end_station_index=TunnExcvDF.index.get_loc(\n",
    "                    TunnExcvDF.loc[(TunnExcvDF[\"Station\"] == end_station),\"Station\"].index[0]) +1\n",
    "                end_station=TunnExcvDF.iloc[end_station_index,TunnExcvDF.columns.get_loc(\"Station\")]\n",
    "                volume_sum=TunnExcvDF.loc[\n",
    "                    ((TunnExcvDF[\"WBScode\"] == i)\n",
    "                     & (TunnExcvDF[\"ExcavationType\"] == j)\n",
    "                     & (TunnExcvDF[\"BoreClass\"] == k)),\"ExcavationVolume\"].sum()\n",
    "                BoQ_list_values.append((i,work_type,j,start_station,end_station,k,volume_sum,volume_unit))\n",
    "                print i, work_type, j, start_station, end_station, k, volume_sum, volume_unit\n",
    "        for k in TunnExcvDF[\"SupportClass\"].unique():\n",
    "            # if DF record with i, j, k (as Support Class) exists:\n",
    "            if ((TunnExcvDF[\"WBScode\"] == i)\n",
    "                & (TunnExcvDF[\"ExcavationType\"] == j)\n",
    "                & (TunnExcvDF[\"SupportClass\"] == k)).any():                    \n",
    "                start_station = min(TunnExcvDF.loc[\n",
    "                    ((TunnExcvDF[\"WBScode\"] == i)\n",
    "                     & (TunnExcvDF[\"ExcavationType\"] == j)),\"Station\"])\n",
    "                end_station = max(TunnExcvDF.loc[\n",
    "                    ((TunnExcvDF[\"WBScode\"] == i)\n",
    "                     & (TunnExcvDF[\"ExcavationType\"] == j)),\"Station\"])\n",
    "                end_station_index=TunnExcvDF.index.get_loc(\n",
    "                    TunnExcvDF.loc[(TunnExcvDF[\"Station\"] == end_station),\"Station\"].index[0]) +1\n",
    "                end_station=TunnExcvDF.iloc[end_station_index,TunnExcvDF.columns.get_loc(\"Station\")]\n",
    "                volume_sum=TunnExcvDF.loc[\n",
    "                    ((TunnExcvDF[\"WBScode\"] == i)\n",
    "                     & (TunnExcvDF[\"ExcavationType\"] == j)\n",
    "                     & (TunnExcvDF[\"SupportClass\"] == k)),\"ExcavationVolume\"].sum()\n",
    "                BoQ_list_values.append((i,work_type,j,start_station,end_station,k,volume_sum,volume_unit))\n",
    "                print i, work_type, j, start_station, end_station, k, volume_sum, volume_unit\n",
    "        for k in TunnExcvDF[\"DisposalClass\"].unique():\n",
    "            # if DF record with i, j, k (as Support Class) exists:\n",
    "            if ((TunnExcvDF[\"WBScode\"] == i)\n",
    "                & (TunnExcvDF[\"ExcavationType\"] == j)\n",
    "                & (TunnExcvDF[\"DisposalClass\"] == k)).any():                    \n",
    "                start_station = min(TunnExcvDF.loc[\n",
    "                    ((TunnExcvDF[\"WBScode\"] == i)\n",
    "                     & (TunnExcvDF[\"ExcavationType\"] == j)),\"Station\"])\n",
    "                end_station = max(TunnExcvDF.loc[\n",
    "                    ((TunnExcvDF[\"WBScode\"] == i)\n",
    "                     & (TunnExcvDF[\"ExcavationType\"] == j)),\"Station\"])\n",
    "                end_station_index=TunnExcvDF.index.get_loc(\n",
    "                    TunnExcvDF.loc[(TunnExcvDF[\"Station\"] == end_station),\"Station\"].index[0]) +1\n",
    "                end_station=TunnExcvDF.iloc[end_station_index,TunnExcvDF.columns.get_loc(\"Station\")]\n",
    "                volume_sum=TunnExcvDF.loc[\n",
    "                    ((TunnExcvDF[\"WBScode\"] == i)\n",
    "                     & (TunnExcvDF[\"ExcavationType\"] == j)\n",
    "                     & (TunnExcvDF[\"DisposalClass\"] == k)),\"DisposalVolume\"].sum()\n",
    "                BoQ_list_values.append((i,work_type,j,start_station,end_station,k,volume_sum,volume_unit))\n",
    "                print i, work_type, j, start_station, end_station, k, volume_sum, volume_unit\n",
    "# check:\n",
    "#print TunnExcvDF.loc[TunnExcvDF[\"ExcavationType\"] == \"TBM\", \"ExcavationVolume\"].sum()\n",
    "#print TunnExcvDF.loc[TunnExcvDF[\"ExcavationType\"] == \"TBM\", \"DisposalVolume\"].sum()\n",
    "\n",
    "BoQ_df =  pd.DataFrame(BoQ_list_values, columns=BoQ_list_headers).round(decimals=3)\n",
    "BoQ_df.to_csv(BoQ, sep=\",\", na_rep=\"NaN\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
